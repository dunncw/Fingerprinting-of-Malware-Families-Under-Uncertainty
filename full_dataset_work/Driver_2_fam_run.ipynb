{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ___ Libraries ___\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from scipy.stats import entropy\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.spatial import distance\n",
    "from scipy.special import kl_div, rel_entr\n",
    "from scipy.stats import entropy\n",
    "\n",
    "from networkx.algorithms.community import girvan_newman, modularity\n",
    "from networkx.algorithms import minimum_spanning_tree\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import community as community_louvain # For Louvain community detection\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATASET  # https://github.com/ocatak/malware_api_class \n",
    "# Preparing the dataset paths\n",
    "data_path = 'PATH_TO_DATA'\n",
    "labels_path = 'PATH_TO_DATA'\n",
    "\n",
    "# Read the data from all_analysis_data.txt and labels.txt\n",
    "with open(data_path, \"r\") as f:\n",
    "    all_traces = f.read().split('\\n')[:-1]  # Array of all untokenized trace documents\n",
    "\n",
    "with open(labels_path, \"r\") as g:\n",
    "    all_labels = g.read().split('\\n')[:-1]  # Remove last blank newline from list\n",
    "\n",
    "# use the labels 'Spyware' and 'Adware' for the first two families\n",
    "selected_labels = ['Spyware', 'Adware']\n",
    "\n",
    "# Select only the traces that have labels in selected_labels\n",
    "all_traces = [trace for label, trace in zip(all_labels, all_traces) if label in selected_labels]\n",
    "\n",
    "# all_labels to be a list of all the corresponding labels for the selected traces\n",
    "all_labels = [label for label in all_labels if label in selected_labels]\n",
    "\n",
    "\n",
    "# # Print the number of selected traces\n",
    "# print(\"Number of selected traces:\", len(all_traces))\n",
    "\n",
    "# # create a dataframe with the traces and labels\n",
    "# import pandas as pd\n",
    "# # Create a dataframe with the traces and labels\n",
    "# df = pd.DataFrame({'traces': all_traces, 'labels': all_labels, 'matrix_index': range(len(all_traces))})\n",
    "# # write the dataframe to a csv file\n",
    "# df.to_csv('output/data_used/actual_traces_and_labels.csv', index=False)\n",
    "# # print the top 10 rows of df\n",
    "# df.head(10)\n",
    "\n",
    "# # # Shuffle and limit data and labels to a smaller subsection for testing\n",
    "# # all_traces, all_labels = shuffle(all_traces, all_labels, random_state=42)\n",
    "# # all_traces = all_traces[:50]\n",
    "# # all_labels = all_labels[:50] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function definitions\n",
    "def get_pairs(trace):\n",
    "    # function to split the trace string into a list\n",
    "    trace = trace.split()\n",
    "    return [(trace[i], trace[i+1]) for i in range(len(trace) - 1)]\n",
    "\n",
    "def construct_adjacency_matrix(trace1, trace2, epsilon=1e-10):\n",
    "    # get the union of the 2 traces\n",
    "    union = list(set(trace1.split()).union(set(trace2.split())))\n",
    "\n",
    "    # construct a matrix of zeros\n",
    "    t1_t2_matrix = np.zeros((len(union), len(union)))\n",
    "    t2_t1_matrix = np.zeros((len(union), len(union)))\n",
    "\n",
    "    # initialize every element in the matrix to 1 besides the diagonal\n",
    "    for i in range(len(union)):\n",
    "        for j in range(len(union)):\n",
    "            if i != j:\n",
    "                t1_t2_matrix[i][j] = 1\n",
    "                t2_t1_matrix[i][j] = 1\n",
    "\n",
    "    # get the frequency of each pair of elements in each trace. remember order matters we only want to count the pair once and going forward in the trace.\n",
    "    pairs1 = get_pairs(trace1)\n",
    "    pairs2 = get_pairs(trace2)\n",
    "\n",
    "    # Get the pair frequencies for each trace\n",
    "    pair_freq1 = Counter([tuple(pair) for pair in pairs1])\n",
    "    pair_freq2 = Counter([tuple(pair) for pair in pairs2])\n",
    "\n",
    "    # update the adjacency matrix with the pair frequencies. add the value of pair_freq[(a,b)] to the matrix at the index of a and b. do not overwrite the values that are already there.\n",
    "    for pair, freq in pair_freq1.items():\n",
    "        t1_t2_matrix[union.index(pair[0])][union.index(pair[1])] += freq\n",
    "    for pair, freq in pair_freq2.items():\n",
    "        t2_t1_matrix[union.index(pair[0])][union.index(pair[1])] += freq\n",
    "\n",
    "    # Add epsilon to the adjacency matrices to avoid divide-by-zero issues in KLD\n",
    "    t1_t2_matrix += epsilon\n",
    "    t2_t1_matrix += epsilon\n",
    "\n",
    "    # normalize the matrix by dividing each row by the sum of the row\n",
    "    t1_t2_matrix = t1_t2_matrix / t1_t2_matrix.sum(axis=1, keepdims=True)\n",
    "    t2_t1_matrix = t2_t1_matrix / t2_t1_matrix.sum(axis=1, keepdims=True)\n",
    "\n",
    "    return t1_t2_matrix, t2_t1_matrix\n",
    "\n",
    "# compute the kl divergence between the 2 traces\n",
    "def compute_kl_divergence(trace1, trace2):\n",
    "    # compute the kl divergence\n",
    "    t1_t2_kld = np.sum(kl_div(trace1.flatten(), trace2.flatten()))\n",
    "    t2_t1_kld = np.sum(kl_div(trace2.flatten(), trace1.flatten()))\n",
    "    return t1_t2_kld, t2_t1_kld\n",
    "\n",
    "# compute the js divergence between the 2 traces\n",
    "def compute_js_divergence(trace1, trace2):\n",
    "    # compute the js divergence\n",
    "    js_divergence = jensenshannon(trace1.flatten(), trace2.flatten())\n",
    "    return js_divergence\n",
    "\n",
    "# Modify the run_all_traces function to fit your current dataset\n",
    "def run_all_traces(all_traces):\n",
    "    num_traces = len(all_traces)\n",
    "    print(f'Number of traces: {num_traces}')\n",
    "\n",
    "    # create empty matrices to store the results\n",
    "    kld_matrix = np.zeros((num_traces, num_traces))\n",
    "    jsd_matrix = np.zeros((num_traces, num_traces))\n",
    "\n",
    "    # iterate over all pairs of traces and compute the kld and jsd\n",
    "    for i in range(num_traces):\n",
    "\n",
    "        print(f'Computing kld and jsd for trace {i}')\n",
    "        for j in range(i+1, num_traces):\n",
    "            # print out the pair of traces we are computing the kld and jsd for\n",
    "            matrix1, matrix2 = construct_adjacency_matrix(all_traces[i], all_traces[j])\n",
    "            kld_12, kld_21 = compute_kl_divergence(matrix1, matrix2)\n",
    "            jsd = compute_js_divergence(matrix1, matrix2)\n",
    "            kld_matrix[i][j] = kld_12\n",
    "            kld_matrix[j][i] = kld_21\n",
    "            jsd_matrix[i][j] = jsd\n",
    "            jsd_matrix[j][i] = jsd\n",
    "\n",
    "    return kld_matrix, jsd_matrix\n",
    "\n",
    "# Run the function\n",
    "kld_matrix, jsd_matrix = run_all_traces(all_traces)\n",
    "\n",
    "# print the results\n",
    "print(\"final results: _____________________________________________________________\")\n",
    "print(\"KL Divergence Matrix: \\n\", kld_matrix)\n",
    "print(\"JS Divergence Matrix: \\n\", jsd_matrix)\n",
    "\n",
    "# write out to csv file \n",
    "np.savetxt(\"output/kld_jsd_matrix/kld_matrix.csv\", kld_matrix, delimiter=\",\")\n",
    "np.savetxt(\"output/kld_jsd_matrix/jsd_matrix.csv\", jsd_matrix, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Print NetworkX version\n",
    "print(\"NetworkX version: \", nx.__version__)\n",
    "\n",
    "# read in the csv file\n",
    "kld_matrix = np.genfromtxt(\"output/kld_jsd_matrix/kld_matrix.csv\", delimiter=\",\")\n",
    "jsd_matrix = np.genfromtxt(\"output/kld_jsd_matrix/jsd_matrix.csv\", delimiter=\",\")\n",
    "\n",
    "# Create a graph from your adjacency matrix.\n",
    "G_jsd = nx.from_numpy_array(jsd_matrix)\n",
    "G_kld = nx.from_numpy_array(kld_matrix)\n",
    "\n",
    "# Compute the minimum spanning tree of the graph using Kruskal's algorithm.\n",
    "mst_jsd = nx.minimum_spanning_tree(G_jsd)\n",
    "mst_kld = nx.minimum_spanning_tree(G_kld)\n",
    "\n",
    "# save the minimum spanning tree to a file\n",
    "nx.write_gml(mst_jsd, \"output/mst/jsd_mst.gml\")\n",
    "nx.write_gml(mst_kld, \"output/mst/kld_mst.gml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ___ MST but with chosen percentage lowest edges instead of just smallest edge ___\n",
    "def create_graph(matrix, percentile):\n",
    "    num_nodes = matrix.shape[0]\n",
    "    k = max(1, int(np.ceil(num_nodes * percentile / 100))) # Number of neighbors\n",
    "\n",
    "    # Initialize the NearestNeighbors\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, metric='precomputed').fit(matrix)\n",
    "\n",
    "    # Get the k-neighbors of each point\n",
    "    distances, indices = nbrs.kneighbors(matrix)\n",
    "\n",
    "    # Create the graph\n",
    "    G = nx.Graph()\n",
    "\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(k):\n",
    "            G.add_edge(str(i), str(indices[i][j]), weight=distances[i][j])\n",
    "            \n",
    "    return G\n",
    "\n",
    "# Read in the csv file\n",
    "kld_matrix = np.genfromtxt(\"output/kld_jsd_matrix/kld_matrix.csv\", delimiter=\",\")\n",
    "jsd_matrix = np.genfromtxt(\"output/kld_jsd_matrix/jsd_matrix.csv\", delimiter=\",\")\n",
    "\n",
    "# create graphs for 1%, 2% and 5%\n",
    "for i in [0.5, 1, 1.5]:\n",
    "    G_jsd = create_graph(jsd_matrix, i)\n",
    "    G_kld = create_graph(kld_matrix, i)\n",
    "    nx.write_gml(G_jsd, f\"output/mst/jsd_{i}.gml\")\n",
    "    nx.write_gml(G_kld, f\"output/mst/kld_{i}.gml\")\n",
    "\n",
    "# # create graphs for 1% - 5% with interval 1%\n",
    "# for i in range(1, 6):\n",
    "#     G_jsd = create_graph(jsd_matrix, i)\n",
    "#     G_kld = create_graph(kld_matrix, i)\n",
    "#     nx.write_gml(G_jsd, f\"output/mst/jsd_{i}.gml\")\n",
    "#     nx.write_gml(G_kld, f\"output/mst/kld_{i}.gml\")\n",
    "\n",
    "# # Create another graph at 10% just for comparison\n",
    "# G_jsd = create_graph(jsd_matrix, 10)\n",
    "# G_kld = create_graph(kld_matrix, 10)\n",
    "# nx.write_gml(G_jsd, f\"output/mst/jsd_10.gml\")\n",
    "# nx.write_gml(G_kld, f\"output/mst/kld_10.gml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('output/mst/*.gml')\n",
    "\n",
    "for file in files:\n",
    "    graph = nx.read_gml(file)\n",
    "    print(f'Graph {file}:')\n",
    "    print(f'Number of nodes: {graph.number_of_nodes()}')\n",
    "    print(f'Number of edges: {graph.number_of_edges()}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time  \n",
    "\n",
    "# Get the list of .gml files in the directory\n",
    "# files = glob.glob('output/mst/*.gml')\n",
    "\n",
    "files = ['mst/kld_0.5.gml',]\n",
    "\n",
    "for file in files:\n",
    "    print(file)\n",
    "    start_time = time.time()  # record the start time\n",
    "    # Read in the graph\n",
    "    mst = nx.read_gml(file)\n",
    "    print(\"read in graph\")\n",
    "    \n",
    "    # Convert MST to undirected graph for community detection\n",
    "    mst_undirected = mst.to_undirected()\n",
    "    print(\"converted to undirected graph\")\n",
    "\n",
    "    # ___ Girvan Newman algorithm ___\n",
    "    # Run the Girvan-Newman algorithm to detect communities\n",
    "    communities_gn = girvan_newman(mst_undirected)\n",
    "    print(\"ran girvan newman\")\n",
    "\n",
    "    # Store communities as dictionaries\n",
    "    communities_gn_dict = {i: sorted(list(c)) for i, c in enumerate(next(communities_gn))}\n",
    "    print(\"Detected communities (Girvan Newman):\", tuple(sorted(c) for c in next(communities_gn)))\n",
    "    \n",
    "    # ___ Louvain algorithm ___\n",
    "    # Run the Louvain algorithm to detect communities\n",
    "    partition_louvain = community_louvain.best_partition(mst_undirected)\n",
    "    print(\"ran louvain\")\n",
    "\n",
    "    # Store communities as dictionaries\n",
    "    communities_louvain = {}\n",
    "    for node, comm in partition_louvain.items():\n",
    "        if comm not in communities_louvain:\n",
    "            communities_louvain[comm] = []\n",
    "        communities_louvain[comm].append(node)\n",
    "    print(\"Detected communities (Louvain):\", tuple(sorted(c) for c in communities_louvain.values()))\n",
    "\n",
    "    communities_louvain_dict = {i: sorted(c) for i, c in communities_louvain.items()}\n",
    "\n",
    "    # Get the base file name without the .gml extension\n",
    "    base_filename = os.path.splitext(os.path.basename(file))[0]\n",
    "    \n",
    "    # Write the dictionaries to disk\n",
    "    with open(f'communities/{base_filename}_gn.json', 'w') as fp:\n",
    "        json.dump(communities_gn_dict, fp)\n",
    "\n",
    "    with open(f'communities/{base_filename}_louvain.json', 'w') as fp:\n",
    "        json.dump(communities_louvain_dict, fp)\n",
    "    \n",
    "    end_time = time.time()  # record the end time\n",
    "    elapsed_time = end_time - start_time  # calculate the elapsed time\n",
    "    print(f\"Elapsed time: {elapsed_time:.2f} seconds\")  # print the elapsed time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import time\n",
    "# # read in the graphs\n",
    "# # mst_jsd = nx.read_gml(\"output/mst/jsd_mst.gml\")\n",
    "# mst_jsd = nx.read_gml(\"output/mst/jsd_1.gml\")\n",
    "# # mst_kld = nx.read_gml(\"output/mst/kld_mst.gml\")\n",
    "\n",
    "# mst = mst_jsd\n",
    "\n",
    "# # Convert MST to undirected graph for community detection\n",
    "# mst_undirected = mst.to_undirected()\n",
    "\n",
    "# # ___ Girvan Newman algorithm ___\n",
    "# # Run the Girvan-Newman algorithm to detect communities\n",
    "# communities_gn = girvan_newman(mst_undirected)\n",
    "\n",
    "# # Compute the modularity of the detected communities\n",
    "# modularity_score_gn = modularity(mst_undirected, next(communities_gn))\n",
    "\n",
    "# # Print the detected communities and modularity score\n",
    "# print(\"Detected communities (Girvan Newman):\", tuple(sorted(c) for c in next(communities_gn)))\n",
    "# print(\"Modularity score (Girvan Newman):\", modularity_score_gn)\n",
    "\n",
    "# # ___ Louvain algorithm ___\n",
    "# # Run the Louvain algorithm to detect communities\n",
    "# partition_louvain = community_louvain.best_partition(mst_undirected)\n",
    "\n",
    "# # Print the detected communities\n",
    "# communities_louvain = {}\n",
    "# for node, comm in partition_louvain.items():\n",
    "#     if comm not in communities_louvain:\n",
    "#         communities_louvain[comm] = []\n",
    "#     communities_louvain[comm].append(node)\n",
    "# print(\"Detected communities (Louvain):\", tuple(sorted(c) for c in communities_louvain.values()))\n",
    "\n",
    "# # store both as dictionaries\n",
    "# communities_gn_dict = {}\n",
    "# for i, comm in enumerate(tuple(sorted(c) for c in next(communities_gn))):\n",
    "#     communities_gn_dict[i] = comm\n",
    "\n",
    "# communities_louvain_dict = {}\n",
    "# for i, comm in enumerate(tuple(sorted(c) for c in communities_louvain.values())):\n",
    "#     communities_louvain_dict[i] = comm\n",
    "\n",
    "# # write these dictionaries to disk in a way they can be read back in as a dictionary\n",
    "# with open('output/communities/communities_gn.json', 'w') as fp:\n",
    "#     json.dump(communities_gn_dict, fp)\n",
    "\n",
    "# # with open('output/communities/communities_louvain.json', 'w') as fp:\n",
    "# #     json.dump(communities_louvain_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the list of .json files in the directory\n",
    "\n",
    "files = glob.glob('communities/*.json')\n",
    "\n",
    "\n",
    "data_path = 'PATH_TO_DATA'\n",
    "labels_path = 'PATH_TO_DATA'\n",
    "\n",
    "with open(labels_path, \"r\") as g:\n",
    "    all_labels = g.read().split('\\n')[:-1]  # Remove last blank newline from list\n",
    "\n",
    "# # use the labels 'Spyware' and 'Adware' for the first two families\n",
    "# selected_labels = ['Spyware', 'Adware']\n",
    "\n",
    "# all labels\n",
    "selected_labels = ['Spyware', 'Adware', 'Downloader', 'Trojan', 'Worms', 'Dropper', 'Virus', 'Backdoor']\n",
    "\n",
    "# all_labels to be a list of all the corresponding labels for the selected traces\n",
    "all_labels = [label for label in all_labels if label in selected_labels]\n",
    "\n",
    "for file in files:\n",
    "    # Read in the graph\n",
    "    with open(file, 'r') as fp:\n",
    "        communities = json.load(fp)\n",
    "    \n",
    "    # _______ confusion matrix ___\n",
    "    \n",
    "    # create a dictionary of the communities\n",
    "    communities_dict = {key: [[int(trace), all_labels[int(trace)]] for trace in communities[key]] for key in communities}\n",
    "    \n",
    "    # create a dictionary with the count, majority label, and key for each community\n",
    "    communities_dict_majority_label = {key: [len(labels), Counter(labels).most_common(1)[0][0], key] for key, labels in [(key, [list[1] for list in communities_dict[key]]) for key in communities_dict]}\n",
    "    \n",
    "    # get the unique labels from the list of all labels\n",
    "    unique_labels = list(set(all_labels))\n",
    "\n",
    "    # print the file name\n",
    "    print(file)\n",
    "    \n",
    "    for label in unique_labels:\n",
    "    # create a list of the communities with the matching majority label\n",
    "        communities = [key for key in communities_dict_majority_label if communities_dict_majority_label[key][1] == label]\n",
    "    # if there is only one community, rename the key to the actual label\n",
    "        if len(communities) == 1:\n",
    "            communities_dict[label] = communities_dict.pop(str(communities[0]))\n",
    "        elif communities:\n",
    "        # find the largest community in the list of communities\n",
    "            largest_community = max(communities, key=lambda k: communities_dict_majority_label[k][0])\n",
    "        # using the community number to find a key in the dictionary communities_dict rename that key to the actual label\n",
    "            communities_dict[label] = communities_dict.pop(str(largest_community))\n",
    "\n",
    "    # Initialize a confusion matrix\n",
    "    predictions = np.append(unique_labels, 'Not Labeled')\n",
    "    confusion_matrix = pd.DataFrame(0, index=unique_labels, columns=predictions)\n",
    "\n",
    "    # Compute the confusion matrix\n",
    "    for key, values in communities_dict.items():\n",
    "        if key in unique_labels:\n",
    "            for value in values:\n",
    "                trace_number, label = value\n",
    "                # if the actual label matches the key its a true positive. they are strings so make sure they match\n",
    "                if str(label) == key:\n",
    "                    # It's a true positive\n",
    "                    confusion_matrix.loc[key, key] += 1\n",
    "                else:\n",
    "                    # It's a false positive\n",
    "                    confusion_matrix.loc[label, key] += 1\n",
    "        if key not in unique_labels:\n",
    "            for value in values:\n",
    "                trace_number, label = value\n",
    "                # false negative\n",
    "                confusion_matrix.loc[label, 'Not Labeled'] += 1\n",
    "    \n",
    "    # Get the base file name without the .json extension\n",
    "    base_filename = os.path.splitext(os.path.basename(file))[0]\n",
    "    \n",
    "    # Write the confusion matrix to a .csv file\n",
    "    confusion_matrix.to_csv(f'confusion_matrix/{base_filename}_confusion_matrix.csv')\n",
    "\n",
    "    #print the file name and the the confusion matrix\n",
    "    print(f'{base_filename} confusion matrix:')\n",
    "    print(confusion_matrix)\n",
    "\n",
    "    # Compute accuracy\n",
    "    accuracy = np.trace(confusion_matrix.values) / np.sum(confusion_matrix.values)\n",
    "    print(f'Accuracy: {accuracy*100:.2f}%')\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10,7))\n",
    "    sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cbar=False)\n",
    "    plt.title(f'{base_filename} confusion matrix\\nAccuracy: {accuracy*100:.2f}%')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.savefig(f'confusion_matrix/{base_filename}_confusion_matrix.png')  # Save the plot as a .png file\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, recall_score, precision_score\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "files = glob.glob('communities/*.json')\n",
    "\n",
    "data_path = 'PATH_TO_DATA'\n",
    "labels_path = 'PATH_TO_DATA'\n",
    "\n",
    "with open(labels_path, \"r\") as g:\n",
    "    all_labels = g.read().split('\\n')[:-1]\n",
    "\n",
    "selected_labels = ['Spyware', 'Adware', 'Downloader', 'Trojan', 'Worms', 'Dropper', 'Virus', 'Backdoor']\n",
    "all_labels = [label for label in all_labels if label in selected_labels]\n",
    "\n",
    "results_table = pd.DataFrame(columns=[\"Dataset\", \"Family\", \"Accuracy\", \"F1 Score\", \"Sensitivity\", \"Specificity\"])\n",
    "\n",
    "for file in files:\n",
    "    with open(file, 'r') as fp:\n",
    "        communities = json.load(fp)\n",
    "\n",
    "    communities_dict = {key: [[int(trace), all_labels[int(trace)]] for trace in communities[key]] for key in communities}\n",
    "    communities_dict_majority_label = {key: [len(labels), Counter(labels).most_common(1)[0][0], key] for key, labels in [(key, [list[1] for list in communities_dict[key]]) for key in communities_dict]}\n",
    "    unique_labels = list(set(all_labels))\n",
    "    print(file)\n",
    "    for label in unique_labels:\n",
    "        communities = [key for key in communities_dict_majority_label if communities_dict_majority_label[key][1] == label]\n",
    "        if len(communities) == 1:\n",
    "            communities_dict[label] = communities_dict.pop(str(communities[0]))\n",
    "        elif communities:\n",
    "            largest_community = max(communities, key=lambda k: communities_dict_majority_label[k][0])\n",
    "            communities_dict[label] = communities_dict.pop(str(largest_community))\n",
    "\n",
    "    predictions = np.append(unique_labels, 'Not Labeled')\n",
    "    confusion_matrix = pd.DataFrame(0, index=unique_labels, columns=predictions)\n",
    "    for key, values in communities_dict.items():\n",
    "        if key in unique_labels:\n",
    "            for value in values:\n",
    "                trace_number, label = value\n",
    "                if str(label) == key:\n",
    "                    confusion_matrix.loc[key, key] += 1\n",
    "                else:\n",
    "                    confusion_matrix.loc[label, key] += 1\n",
    "        if key not in unique_labels:\n",
    "            for value in values:\n",
    "                trace_number, label = value\n",
    "                confusion_matrix.loc[label, 'Not Labeled'] += 1\n",
    "\n",
    "    base_filename = os.path.splitext(os.path.basename(file))[0]\n",
    "    confusion_matrix.to_csv(f'confusion_matrix/{base_filename}_confusion_matrix.csv')\n",
    "    print(f'{base_filename} confusion matrix:')\n",
    "    print(confusion_matrix)\n",
    "\n",
    "    # Iterate over families (labels)\n",
    "    for family in unique_labels:\n",
    "        try:\n",
    "            y_true = [1 if l == family else 0 for l in all_labels]\n",
    "            y_pred = [1 if key == family and str(label) == key else 0 for key, values in communities_dict.items() for value in values]\n",
    "            accuracy = accuracy_score(y_true, y_pred)\n",
    "            f1 = f1_score(y_true, y_pred)\n",
    "            sensitivity = recall_score(y_true, y_pred)  # Sensitivity is the same as recall\n",
    "            specificity = recall_score([0 if i == 1 else 1 for i in y_true], [0 if i == 1 else 1 for i in y_pred])\n",
    "\n",
    "            results_table = results_table.append({\"Dataset\": base_filename, \"Family\": family, \"Accuracy\": accuracy, \"F1 Score\": f1, \"Sensitivity\": sensitivity, \"Specificity\": specificity}, ignore_index=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not compute metrics for family {family} in {base_filename}: {str(e)}\")\n",
    "\n",
    "    plt.figure(figsize=(10,7))\n",
    "    sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cbar=False)\n",
    "    plt.title(f'{base_filename} confusion matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.savefig(f'confusion_matrix/{base_filename}_confusion_matrix.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "# Write the results table to a .csv file\n",
    "results_table.to_csv(\"results_table.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read in all the confusion matrices\n",
    "# files = glob.glob('output/confusion_matrix/*.csv')\n",
    "\n",
    "# # for each confusion matrix render it as a heatmap\n",
    "# for file in files:\n",
    "#     # read in the confusion matrix\n",
    "#     confusion_matrix = pd.read_csv(file, index_col=0)\n",
    "    \n",
    "#     # get the base file name without the .csv extension\n",
    "#     base_filename = os.path.splitext(os.path.basename(file))[0]\n",
    "    \n",
    "#     # plot the confusion matrix as a heatmap\n",
    "#     fig, ax = plt.subplots(figsize=(5.5, 3.7))\n",
    "#     sns.heatmap(confusion_matrix, annot=True, cmap='Blues', fmt='g')\n",
    "#     ax.set_title(f'{base_filename}')\n",
    "#     ax.set_xlabel('Predicted label')\n",
    "#     ax.set_ylabel('True label')\n",
    "#     plt.savefig(f'output/plots/{base_filename}_confusion_matrix.png')\n",
    "#     plt.close()\n",
    "    \n",
    "#     # print the confusion matrix\n",
    "#     print(f'{base_filename} confusion matrix:')\n",
    "#     print(confusion_matrix)\n",
    "#     print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read in all the confusion matrices\n",
    "files = glob.glob('confusion_matrix/*.csv')\n",
    "\n",
    "# Initialize dictionaries\n",
    "metrics = {}\n",
    "\n",
    "# Calculate the sensitivity and specificity for each confusion matrix and store them in the dictionary\n",
    "for file in files:\n",
    "    # Read in the confusion matrix\n",
    "    confusion_matrix = pd.read_csv(file, index_col=0)\n",
    "    \n",
    "    # Get the base file name without the .csv extension\n",
    "    base_filename = os.path.splitext(os.path.basename(file))[0]\n",
    "    \n",
    "    # Get the method and type used for community detection\n",
    "    method = base_filename.split('_')[2]\n",
    "    type_ = base_filename.split('_')[0]\n",
    "    \n",
    "    # Get the size of the confusion matrix\n",
    "    size = base_filename.split('_')[1]\n",
    "    \n",
    "    # Calculate the sensitivity and specificity for each family\n",
    "    for family in ['Spyware', 'Adware']:\n",
    "        sensitivity = confusion_matrix.loc[family, family] / confusion_matrix.loc[family].sum()\n",
    "        specificity = confusion_matrix.drop(family).drop(family, axis=1).sum().sum() / confusion_matrix.drop(family).sum().sum()\n",
    "        \n",
    "        # Store the sensitivity and specificity in a dictionary\n",
    "        key = (type_, method, family)\n",
    "        metrics[key] = metrics.get(key, []) + [[size, sensitivity, specificity]]\n",
    "\n",
    "# Define plot parameters\n",
    "methods = ['gn', 'louvain']\n",
    "families = ['Spyware', 'Adware']\n",
    "types = ['kld', 'jsd']\n",
    "\n",
    "# Initialize the color and linestyle dictionary\n",
    "colors = {'sensitivity': {'kld': {'Spyware': ['darkblue', '-'], 'Adware': ['darkblue', '--']}, \n",
    "                           'jsd': {'Spyware': ['lightblue', '-'], 'Adware': ['lightblue', '--']}}, \n",
    "          'specificity': {'kld': {'Spyware': ['darkred', '-'], 'Adware': ['darkred', '--']},\n",
    "                          'jsd': {'Spyware': ['lightcoral', '-'], 'Adware': ['lightcoral', '--']}}}\n",
    "\n",
    "# Generate plot for each method\n",
    "for method in methods:\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    for family in families:\n",
    "        for type_ in types:\n",
    "            key = (type_, method, family)\n",
    "            if key in metrics:\n",
    "                values = metrics[key]\n",
    "                size = [v[0] for v in values]\n",
    "                sensitivity = [v[1] for v in values]\n",
    "                specificity = [v[2] for v in values]\n",
    "\n",
    "                # Sort the lists according to size\n",
    "                # Consider 'mst' as -1 for sorting purposes\n",
    "                sorted_lists = sorted(zip(size, sensitivity, specificity), key=lambda x: -1 if x[0] == 'mst' else float(x[0]))\n",
    "                size, sensitivity, specificity = (list(t) for t in zip(*sorted_lists))\n",
    "\n",
    "                # Plot sensitivity and specificity for each type and family\n",
    "                ax.plot(size, sensitivity, color=colors['sensitivity'][type_][family][0], linestyle=colors['sensitivity'][type_][family][1], label=f'{family}_{type_}_Sensitivity')\n",
    "                ax.plot(size, specificity, color=colors['specificity'][type_][family][0], linestyle=colors['specificity'][type_][family][1], label=f'{family}_{type_}_Specificity')\n",
    "    \n",
    "    ax.set_title(f'Method: {method}')\n",
    "    ax.set_xlabel('Size')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "    plt.savefig(f'plots/{method}_sensitivity_specificity.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in all the confusion matrices create one table with all the results\n",
    "files = glob.glob('output/confusion_matrix/*.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write all_matrices to excel\n",
    "all_matrices.to_excel('output/confusion_matrix/all_matrices.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ___ sanity check ___\n",
    "# # read in the predicted communities which are json files\n",
    "# with open('output/communities/communities_gn.json', 'r') as fp:\n",
    "#     communities_gn = json.load(fp)\n",
    "\n",
    "# with open('output/communities/communities_louvain.json', 'r') as fp:\n",
    "#     communities_louvain = json.load(fp)\n",
    "\n",
    "# # ensure there are no duplicates in communities_gn\n",
    "# from collections import Counter\n",
    "\n",
    "# print(\"Number of families deteced via gn:\", len(communities_gn))\n",
    "# print(\"Number of families deteced via louvain:\", len(communities_louvain))\n",
    "\n",
    "# total_count = 0\n",
    "# for key in communities_gn:\n",
    "#     sublist_count = len(communities_gn[key])\n",
    "#     total_count += sublist_count\n",
    "\n",
    "# print(\"Total number of elements in all sublists gn:\", total_count)\n",
    "\n",
    "# seen_values = set()\n",
    "# for key in communities_gn:\n",
    "#     for value in communities_gn[key]:\n",
    "#         if value in seen_values:\n",
    "#             print(\"Duplicate value found:\", value)\n",
    "#         else:\n",
    "#             seen_values.add(value)\n",
    "\n",
    "# print(\"Number of unique values gn:\", len(seen_values))\n",
    "\n",
    "# # do the same for communities_louvain\n",
    "\n",
    "# total_count = 0\n",
    "# for key in communities_louvain:\n",
    "#     sublist_count = len(communities_louvain[key])\n",
    "#     total_count += sublist_count\n",
    "\n",
    "# print(\"Total number of elements in all sublists louvain:\", total_count)\n",
    "\n",
    "# seen_values = set()\n",
    "# for key in communities_louvain:\n",
    "#     for value in communities_louvain[key]:\n",
    "#         if value in seen_values:\n",
    "#             print(\"Duplicate value found:\", value)\n",
    "#         else:\n",
    "#             seen_values.add(value)\n",
    "\n",
    "# print(\"Number of unique values louvain:\", len(seen_values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ___ confusion matrix ___\n",
    "\n",
    "# with open(labels_path, \"r\") as g:\n",
    "#     all_labels = g.read().split('\\n')[:-1]  # Remove last blank newline from list\n",
    "\n",
    "# # use the labels 'Spyware' and 'Adware' for the first two families\n",
    "# selected_labels = ['Spyware', 'Adware']\n",
    "\n",
    "# # all_labels to be a list of all the corresponding labels for the selected traces\n",
    "# all_labels = [label for label in all_labels if label in selected_labels]\n",
    "\n",
    "# # read in the predicted communities which are json files\n",
    "# with open('output/communities/communities_gn.json', 'r') as fp:\n",
    "#     communities_gn = json.load(fp)\n",
    "\n",
    "# with open('output/communities/communities_louvain.json', 'r') as fp:\n",
    "#     communities_louvain = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ________ confusion matrix for louvain\n",
    "# # create a dictionary with the communities from the Louvain algorithm\n",
    "# communities_louvain_dict = {key: [[int(trace), actual_traces_and_labels.loc[actual_traces_and_labels['matrix_index'] == int(trace), 'labels'].iloc[0]] for trace in communities_louvain[key]] for key in communities_louvain}\n",
    "\n",
    "# # create a dictionary with the count, majority label, and key for each community\n",
    "# communities_louvain_dict_majority_label = {key: [len(labels), Counter(labels).most_common(1)[0][0], key] for key, labels in [(key, [list[1] for list in communities_louvain_dict[key]]) for key in communities_louvain_dict]}\n",
    "\n",
    "# # create a list of all the unique actual labels\n",
    "# actual_labels = actual_traces_and_labels['labels'].unique()\n",
    "\n",
    "# for label in actual_labels:\n",
    "#     # create a list of the communities with the matching majority label\n",
    "#     communities = [key for key in communities_louvain_dict_majority_label if communities_louvain_dict_majority_label[key][1] == label]\n",
    "#     # find the largest community in the list of communities\n",
    "#     largest_community = max(communities, key=lambda k: communities_louvain_dict_majority_label[k][0])\n",
    "#     # print out the actual label, the community number, the count, and the majority label\n",
    "#     print(\"actual label: {label}, community number: {community}, count: {count}, majority label: {majority_label}\".format(label=label, community=largest_community, count=communities_louvain_dict_majority_label[largest_community][0], majority_label=communities_louvain_dict_majority_label[largest_community][1]))\n",
    "#     # using the community number to find a key in the dictionary communities_louvain_dict rename that key to the actual label\n",
    "#     communities_louvain_dict[label] = communities_louvain_dict.pop(str(largest_community))\n",
    "\n",
    "# # Let's first initialize a confusion matrix as a DataFrame with 'actual_labels' as both the indices and columns\n",
    "# # create an ndarray of actual_labels and add one more element called 'not_in_community'\n",
    "# predictions = np.append(actual_labels, 'not_in_majority_community')\n",
    "# confusion_matrix = pd.DataFrame(0, index=actual_labels, columns=predictions)\n",
    "\n",
    "# # Now let's go through 'communities_louvain_dict'\n",
    "# for key, values in communities_louvain_dict.items():\n",
    "#     if key in actual_labels:\n",
    "#         for value in values:\n",
    "#             trace_number, actual_label = value\n",
    "#             # if the actual label matches the key its a true positive. they are strings so make sure they match\n",
    "#             if str(actual_label) == key:\n",
    "#                 # It's a true positive\n",
    "#                 confusion_matrix.loc[key, key] += 1\n",
    "#             else:\n",
    "#                 # It's a false positive\n",
    "#                 confusion_matrix.loc[actual_label, key] += 1\n",
    "#     if key not in actual_labels:\n",
    "#         for value in values:\n",
    "#             trace_number, actual_label = value\n",
    "#             # false negative\n",
    "#             confusion_matrix.loc[actual_label, 'not_in_majority_community'] += 1\n",
    "\n",
    "\n",
    "# # Print the confusion matrix\n",
    "# print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # _______ confusion matrix for gn\n",
    "\n",
    "# # create a dictionary of the communities\n",
    "# communities_gn_dict = {key: [[int(trace), all_labels[int(trace)]] for trace in communities_gn[key]] for key in communities_gn}\n",
    "\n",
    "# # create a dictionary with the count, majority label, and key for each community\n",
    "# communities_gn_dict_majority_label = {key: [len(labels), Counter(labels).most_common(1)[0][0], key] for key, labels in [(key, [list[1] for list in communities_gn_dict[key]]) for key in communities_gn_dict]}\n",
    "\n",
    "# # get the unquie labels from the list of all labels\n",
    "# unique_labels = list(set(all_labels))\n",
    "\n",
    "# for label in unique_labels:\n",
    "#     # create a list of the communities with the matching majority label\n",
    "#     communities = [key for key in communities_gn_dict_majority_label if communities_gn_dict_majority_label[key][1] == label]\n",
    "#     # find the largest community in the list of communities\n",
    "#     largest_community = max(communities, key=lambda k: communities_gn_dict_majority_label[k][0])\n",
    "#     # print out the actual label, the community number, the count, and the majority label\n",
    "#     print(\"actual label: {label}, community number: {community}, count: {count}, majority label: {majority_label}\".format(label=label, community=largest_community, count=communities_gn_dict_majority_label[largest_community][0], majority_label=communities_gn_dict_majority_label[largest_community][1]))\n",
    "#     # using the community number to find a key in the dictionary communities_louvain_dict rename that key to the actual label\n",
    "#     communities_gn_dict[label] = communities_gn_dict.pop(str(largest_community))\n",
    "\n",
    "# # Let's first initialize a confusion matrix as a DataFrame with 'actual_labels' as both the indices and columns\n",
    "# # create an ndarray of actual_labels and add one more element called 'not_in_community'\n",
    "# predictions = np.append(unique_labels, 'not_in_majority_community')\n",
    "# confusion_matrix = pd.DataFrame(0, index=unique_labels, columns=predictions)\n",
    "\n",
    "# # Now let's go through 'communities_louvain_dict'\n",
    "# for key, values in communities_gn_dict.items():\n",
    "#     if key in unique_labels:\n",
    "#         for value in values:\n",
    "#             trace_number, label = value\n",
    "#             # if the actual label matches the key its a true positive. they are strings so make sure they match\n",
    "#             if str(label) == key:\n",
    "#                 # It's a true positive\n",
    "#                 confusion_matrix.loc[key, key] += 1\n",
    "#             else:\n",
    "#                 # It's a false positive\n",
    "#                 confusion_matrix.loc[label, key] += 1\n",
    "#     if key not in unique_labels:\n",
    "#         for value in values:\n",
    "#             trace_number, label = value\n",
    "#             # false negative\n",
    "#             confusion_matrix.loc[label, 'not_in_majority_community'] += 1\n",
    "\n",
    "\n",
    "# # Print the confusion matrix\n",
    "# print(confusion_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
