{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['communities\\\\jsd_0.5_gn.json', 'communities\\\\jsd_0.5_louvain.json', 'communities\\\\jsd_mst_gn.json', 'communities\\\\jsd_mst_louvain.json', 'communities\\\\kld_mst_gn.json', 'communities\\\\kld_mst_louvain.json']\n",
      "communities\\jsd_0.5_gn.json\n",
      "Trojan: 6993\n",
      "Virus: 114\n",
      "communities\\jsd_0.5_louvain.json\n",
      "Worms: 1059\n",
      "Trojan: 57\n",
      "Backdoor: 1044\n",
      "Downloader: 469\n",
      "Virus: 1034\n",
      "Adware: 149\n",
      "Dropper: 635\n",
      "communities\\jsd_mst_gn.json\n",
      "Worms: 3606\n",
      "Downloader: 3501\n",
      "communities\\jsd_mst_louvain.json\n",
      "Spyware: 106\n",
      "Worms: 207\n",
      "Trojan: 115\n",
      "Backdoor: 134\n",
      "Downloader: 115\n",
      "Virus: 178\n",
      "Adware: 65\n",
      "Dropper: 106\n",
      "communities\\kld_mst_gn.json\n",
      "Backdoor: 3371\n",
      "Downloader: 3736\n",
      "communities\\kld_mst_louvain.json\n",
      "Spyware: 81\n",
      "Worms: 62\n",
      "Trojan: 82\n",
      "Backdoor: 70\n",
      "Downloader: 110\n",
      "Virus: 106\n",
      "Adware: 94\n",
      "Dropper: 90\n"
     ]
    }
   ],
   "source": [
    "# ### this should only be run once to update the communities json files to create desired formating\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_path = r'C:\\Users\\cayde\\Desktop\\Grad_School_stuff\\DataBaseManagement\\Project\\Analysis_Scripts\\data\\mal-api-2019\\all_analysis_data.txt'\n",
    "labels_path = r'C:\\Users\\cayde\\Desktop\\Grad_School_stuff\\DataBaseManagement\\Project\\Analysis_Scripts\\data\\mal-api-2019\\labels.txt'\n",
    "\n",
    "# Read the data from all_analysis_data.txt and labels.txt\n",
    "with open(data_path, \"r\") as f:\n",
    "    all_traces = f.read().split('\\n')[:-1]  # Array of all untokenized trace documents\n",
    "\n",
    "with open(labels_path, \"r\") as g:\n",
    "    all_labels = g.read().split('\\n')[:-1]  # Remove last blank newline from list\n",
    "\n",
    "\n",
    "import glob\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "# Get the list of .json files in the directory\n",
    "files = glob.glob(r'communities\\*.json')\n",
    "# print a list of the files\n",
    "print(files)\n",
    "# files = ['output/communities/kld_mst_louvain.json']\n",
    "\n",
    "for file in files:\n",
    "    print(file)\n",
    "    # Read in the graph\n",
    "    with open(file, 'r') as fp:\n",
    "        communities = json.load(fp)\n",
    "    \n",
    "    # create a dictionary of the communities\n",
    "    communities_dict = {key: [[int(trace), all_labels[int(trace)]] for trace in communities[key]] for key in communities}\n",
    "    \n",
    "    # create a dictionary with the count, majority label, and key for each community\n",
    "    communities_dict_majority_label = {key: [len(labels), Counter(labels).most_common(1)[0][0], key] for key, labels in [(key, [list[1] for list in communities_dict[key]]) for key in communities_dict]}\n",
    "    \n",
    "    # get the unique labels from the list of all labels\n",
    "    unique_labels = list(set(all_labels))\n",
    "    \n",
    "    updated_dict = {}\n",
    "\n",
    "    for label in unique_labels:\n",
    "        # create a list of the communities with the matching majority label\n",
    "        communities = [key for key in communities_dict_majority_label if communities_dict_majority_label[key][1] == label]\n",
    "        # if there is only one community, rename the key to the actual label\n",
    "        if len(communities) == 1:\n",
    "            updated_dict[label] = communities_dict[str(communities[0])]\n",
    "        elif communities:\n",
    "            # find the largest community in the list of communities\n",
    "            largest_community = max(communities, key=lambda k: communities_dict_majority_label[k][0])\n",
    "            # using the community number to find a key in the dictionary communities_dict rename that key to the actual label\n",
    "            updated_dict[label] = communities_dict[str(largest_community)]\n",
    "\n",
    "    # print a list of the keys in the updated_dict. with nice formatting\n",
    "    print('\\n'.join([str(key) + ': ' + str(len(updated_dict[key])) for key in updated_dict]))\n",
    "\n",
    "    # grab the file name from the path\n",
    "    file_name = file.split('\\\\')[-1]\n",
    "    # write out new json file with the updated_dict to output/communities\n",
    "    with open('communities_with_majority/' + file_name, 'w') as fp:\n",
    "        json.dump(updated_dict, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run from here on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all libs\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "import glob\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_path = r'C:\\Users\\cayde\\Desktop\\Grad_School_stuff\\DataBaseManagement\\Project\\Analysis_Scripts\\data\\mal-api-2019\\all_analysis_data.txt'\n",
    "labels_path = r'C:\\Users\\cayde\\Desktop\\Grad_School_stuff\\DataBaseManagement\\Project\\Analysis_Scripts\\data\\mal-api-2019\\labels.txt'\n",
    "\n",
    "# Read the data from all_analysis_data.txt and labels.txt\n",
    "with open(data_path, \"r\") as f:\n",
    "    all_traces = f.read().split('\\n')[:-1]  # Array of all untokenized trace documents\n",
    "\n",
    "with open(labels_path, \"r\") as g:\n",
    "    all_labels = g.read().split('\\n')[:-1]  # Remove last blank newline from list\n",
    "\n",
    "# use the labels 'Spyware' and 'Adware' for the first two families\n",
    "selected_labels = ['Spyware', 'Adware', 'Downloader', 'Trojan', 'Worms', 'Dropper', 'Virus', 'Backdoor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_dataset = pd.read_csv(\"ml_datasets\\\\og_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jsd_0.5_gn', 'jsd_0.5_louvain', 'jsd_mst_gn', 'jsd_mst_louvain', 'kld_mst_gn', 'kld_mst_louvain']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "dictionary_names = []  # create an empty list to store the dictionary names\n",
    "\n",
    "for file in os.listdir('communities_with_majority'):\n",
    "    if file.endswith('.json'):\n",
    "        with open('communities_with_majority/' + file) as json_file:\n",
    "            dictionary_name = file[:-5]  # remove the .json extension from the filename\n",
    "            locals()[dictionary_name] = json.load(json_file)  # create a new dictionary with the filename as the key\n",
    "            dictionary_names.append(dictionary_name)  # add the dictionary name to the list\n",
    "\n",
    "print(dictionary_names)  # print the list of dictionary names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jsd_0.5_gn\n",
      "Number of traces selected: 7107\n",
      "Number of rows in the dataset: 7107\n",
      "Are all column names unique? True\n",
      "Are there any rows with all null values? False\n",
      "jsd_0.5_louvain\n",
      "Number of traces selected: 4447\n",
      "Number of rows in the dataset: 4447\n",
      "Are all column names unique? True\n",
      "Are there any rows with all null values? False\n",
      "jsd_mst_gn\n",
      "Number of traces selected: 7107\n",
      "Number of rows in the dataset: 7107\n",
      "Are all column names unique? True\n",
      "Are there any rows with all null values? False\n",
      "jsd_mst_louvain\n",
      "Number of traces selected: 1026\n",
      "Number of rows in the dataset: 1026\n",
      "Are all column names unique? True\n",
      "Are there any rows with all null values? False\n",
      "kld_mst_gn\n",
      "Number of traces selected: 7107\n",
      "Number of rows in the dataset: 7107\n",
      "Are all column names unique? True\n",
      "Are there any rows with all null values? False\n",
      "kld_mst_louvain\n",
      "Number of traces selected: 695\n",
      "Number of rows in the dataset: 695\n",
      "Are all column names unique? True\n",
      "Are there any rows with all null values? False\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# define a function to get pairs from a string\n",
    "def get_pairs(s):\n",
    "    words = s.split()\n",
    "    return (words[i] + \" \" + words[i+1] for i in range(len(words) - 1))\n",
    "\n",
    "# Read and process each community dictionary\n",
    "for dictionary_name in dictionary_names:\n",
    "    print(dictionary_name)\n",
    "    # Access the current dictionary\n",
    "    communities = locals()[dictionary_name]\n",
    "\n",
    "    # Step 1: Extract all pairs from selected traces and construct a dictionary with their frequencies\n",
    "    selected_trace_ids = []\n",
    "    trace_label_mapping = {}  # new dictionary to store the trace_id to label mapping\n",
    "\n",
    "    # Check each community in selected_labels\n",
    "    for label in selected_labels:\n",
    "        # Check if the label exists in the communities dictionary\n",
    "        if label not in communities:\n",
    "            continue\n",
    "\n",
    "        # Iterate over the traces in this community\n",
    "        for trace_id, _ in communities[label]:\n",
    "            # Store the trace_id\n",
    "            selected_trace_ids.append(trace_id)\n",
    "            # Store the label for this trace_id\n",
    "            trace_label_mapping[trace_id] = label\n",
    "\n",
    "    print(\"Number of traces selected: {}\".format(len(selected_trace_ids)))\n",
    "\n",
    "    # Step 2: Construct the dataset\n",
    "    data = []\n",
    "    for trace_id in selected_trace_ids:\n",
    "        # Get the trace for this id\n",
    "        trace = all_traces[int(trace_id)]  # Assuming the trace_id can be used as index\n",
    "\n",
    "        # Get the pairs in this trace\n",
    "        pairs = get_pairs(trace)\n",
    "        pair_freq = Counter(pairs)  # Using Counter to calculate pair frequencies\n",
    "\n",
    "        # Create a row for this trace\n",
    "        row = {\n",
    "            \"trace_id\": trace_id,\n",
    "            \"label\": trace_label_mapping[trace_id],  # Get label for trace from the new trace_label_mapping dictionary\n",
    "        }\n",
    "\n",
    "        # Add the pair frequencies to the row\n",
    "        row.update(pair_freq)\n",
    "\n",
    "        # Append this row to the data\n",
    "        data.append(row)\n",
    "\n",
    "    print(\"Number of rows in the dataset: {}\".format(len(data)))\n",
    "\n",
    "    # Step 3: Construct a pandas dataframe\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    print(\"Are all column names unique? {}\".format(df.columns.is_unique))\n",
    "    print(\"Are there any rows with all null values? {}\".format(df.isnull().all(axis=1).any()))\n",
    "\n",
    "    # Save the dataframe to a csv file\n",
    "    df.to_csv(f'ml_datasets/{dictionary_name}_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jsd_0.5_gn\n",
      "jsd_0.5_louvain\n",
      "jsd_mst_gn\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jsd_0.5_gn\n",
      "29\n",
      "jsd_0.5_louvain\n",
      "39\n",
      "jsd_mst_gn\n",
      "15\n",
      "jsd_mst_louvain\n",
      "4\n",
      "kld_mst_gn\n",
      "15\n",
      "kld_mst_louvain\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "# all libs\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "from collections import Counter\n",
    "\n",
    "data_path = r'C:\\Users\\cayde\\Desktop\\Grad_School_stuff\\DataBaseManagement\\Project\\Analysis_Scripts\\data\\mal-api-2019\\all_analysis_data.txt'\n",
    "labels_path = r'C:\\Users\\cayde\\Desktop\\Grad_School_stuff\\DataBaseManagement\\Project\\Analysis_Scripts\\data\\mal-api-2019\\labels.txt'\n",
    "\n",
    "# Read the data from all_analysis_data.txt and labels.txt\n",
    "with open(data_path, \"r\") as f:\n",
    "    all_traces = f.read().split('\\n')[:-1]\n",
    "\n",
    "with open(labels_path, \"r\") as g:\n",
    "    all_labels = g.read().split('\\n')[:-1]\n",
    "\n",
    "selected_labels = ['Spyware', 'Adware', 'Downloader', 'Trojan', 'Worms', 'Dropper', 'Virus', 'Backdoor']\n",
    "og_dataset = pd.read_csv(\"ml_datasets\\\\og_dataset.csv\")\n",
    "\n",
    "dictionary_names = []\n",
    "\n",
    "for file in os.listdir('communities_with_majority'):\n",
    "    if file.endswith('.json'):\n",
    "        with open('communities_with_majority/' + file) as json_file:\n",
    "            dictionary_name = file[:-5]\n",
    "            locals()[dictionary_name] = json.load(json_file)\n",
    "            dictionary_names.append(dictionary_name)\n",
    "\n",
    "# define a function to get pairs from a string\n",
    "def get_pairs(s):\n",
    "    words = s.split()\n",
    "    return (words[i] + \" \" + words[i+1] for i in range(len(words) - 1))\n",
    "\n",
    "# Read and process each community dictionary\n",
    "for dictionary_name in dictionary_names:\n",
    "    print(dictionary_name)\n",
    "    communities = locals()[dictionary_name]\n",
    "\n",
    "    selected_trace_ids = []\n",
    "    trace_label_mapping = {}\n",
    "    top_10_attributes = []\n",
    "\n",
    "    for label in selected_labels:\n",
    "        if label not in communities:\n",
    "            continue\n",
    "\n",
    "        for trace_id, _ in communities[label]:\n",
    "            selected_trace_ids.append(trace_id)\n",
    "            trace_label_mapping[trace_id] = label\n",
    "\n",
    "    data = []\n",
    "    for trace_id in selected_trace_ids:\n",
    "        trace = all_traces[int(trace_id)]\n",
    "        pairs = get_pairs(trace)\n",
    "        pair_freq = Counter(pairs)\n",
    "\n",
    "        row = {\n",
    "            \"trace_id\": trace_id,\n",
    "            \"label\": trace_label_mapping[trace_id],\n",
    "        }\n",
    "        row.update(pair_freq)\n",
    "        data.append(row)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Find top 10 attributes for each label\n",
    "    for label in selected_labels:\n",
    "        df_label = df[df['label'] == label]  # filter rows for the specific label\n",
    "        # drop columns with all null values, then sum each column, sort them, and get the top 10\n",
    "        top_attributes = df_label.dropna(axis=1).sum(numeric_only=True).sort_values(ascending=False).head(15).index.tolist()\n",
    "        top_10_attributes.extend(top_attributes)  # Add the top 10 attributes for this label to the list\n",
    "\n",
    "    # Get unique top attributes\n",
    "    top_10_attributes = list(set(top_10_attributes))\n",
    "\n",
    "    # print the len of top_10_attributes list\n",
    "    print(len(top_10_attributes))\n",
    "\n",
    "    # Filter the og_dataset\n",
    "    og_filtered_dataset = og_dataset[['trace_id', 'label'] + top_10_attributes]  # add 'trace_id' and 'label' columns to the filtered dataset\n",
    "    og_filtered_dataset.to_csv(f'new_dataset/og_{dictionary_name}_dataset.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
